{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.36 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "print (brain_name)\n",
    "print (brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import Dict_Hyperparams as P\n",
    "import Util as U\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from PPO_Agent import MasterAgent, Rollout, ActorAndCritic, SubNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Process\n",
    "\n",
    "The TrainingSession class trains the agent while monitoring the progress of the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainingSession():\n",
    "    \n",
    "    def __init__(self, agent, num_workers):\n",
    "        self.agent = agent\n",
    "        self.num_workers = num_workers\n",
    "        self.online_rewards = np.zeros(num_workers)\n",
    "        self.mean_last_100 = 0\n",
    "        self.mean_scores = []\n",
    "\n",
    "    def train_ppo(self, agent, target_average_score, max_episodes=300):\n",
    "        print(\"Attempting to reach 100 episode trailing average of {:.2f} in under {} episodes.\".format(target_average_score, max_episodes))\n",
    "        print(\"Rollout length: %s\" % P.ROLLOUT_LENGTH)\n",
    "        print(\"GRADIENT_CLIP %s\" % P.GRADIENT_CLIP)\n",
    "        print(\"PPO_RATIO_CLIP %s\" % P.PPO_RATIO_CLIP)\n",
    "        print(\"GAE_LAMBDA %s\" % P.GAE_LAMBDA)\n",
    "        #agent.save_weights()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment   \n",
    "        self.num_episodes = 0\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.episode_scores = [list() for i in range(num_agents)]\n",
    "        self.episodes_finished = 0\n",
    "        self.target_average_score = target_average_score\n",
    "        while True:\n",
    "            mean_last_100 = self.run_rollout(agent, env_info)\n",
    "            agent.process_rollout(states)\n",
    "            \n",
    "            #print(\"\\rEpisode {}\\tLast 100: {:.2f}\".format(self.num_episodes, mean_last_100))\n",
    "            if mean_last_100 > target_average_score:\n",
    "                print(\"Reached target! mean_last_100 %s\" % mean_last_100)\n",
    "                agent.save_weights() \n",
    "                break\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Failed to reach target in {} episodes.\".format(self.num_episodes))\n",
    "                break\n",
    "        return self.mean_scores \n",
    "    \n",
    "    def get_actions_from_policy(self, states):\n",
    "        actions = self.agent.get_prediction(states)          # Run the policy \n",
    "        actions = U.to_np(actions)                        # Extract actions\n",
    "        actions = np.clip(actions, -1, 1)               # all actions between -1 and 1\n",
    "        return actions\n",
    "    \n",
    "    def run_ppo(self, agent, max_episodes=5):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.num_episodes = 0\n",
    "        while True:\n",
    "            mean_score_over_agents = self.run_rollout(agent, env_info)\n",
    "            self.num_episodes += 1\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Policy failed to reach target in %s\" % max_episodes)\n",
    "                break\n",
    "    \n",
    "    def step_environment(self, env, actions):\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment        \n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished for each agent\n",
    "        return (next_states, rewards, dones)\n",
    "\n",
    "    \n",
    "    '''                           \n",
    "    def run_episode(self, env_info):\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        while True:\n",
    "            actions = self.get_actions_from_policy(states)\n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "    ''' \n",
    "\n",
    "    def run_rollout(self, agent, env_info):\n",
    "        #print(\"Run rollout\")\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        for t in range(P.ROLLOUT_LENGTH):\n",
    "            actions = self.get_actions_from_policy(states)            \n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "        \n",
    "            # Save rewards\n",
    "            self.online_rewards += rewards                          # Accumulate ongoing (un-normalized) rewards for each agent\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:                                       # For a worker whose episode is done...\n",
    "                    #print(\"Worker %s finished at timestep %s\" % (i, t))\n",
    "                    self.end_episode(i, self.online_rewards[i])\n",
    "                    self.online_rewards[i] = 0                 # Reset accumulated reward for next episode\n",
    "                    self.mean_last_100 = np.mean(self.last_100_scores)\n",
    "                    if self.mean_last_100 > self.target_average_score:\n",
    "                        print(\"Breaking\")\n",
    "                        break\n",
    "            #print(\"%s step\" % t)            \n",
    "            agent.step(states, actions, rewards, next_states, dones) # Teach the agent \n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "\n",
    "        #print(\"Steps in rollout: {}\".format(t+1))        \n",
    "        return self.mean_last_100    \n",
    "\n",
    "    def end_episode(self, agent_index, score):\n",
    "        self.episode_scores[agent_index].append(score)   # Save the reward they accumulated in the episode\n",
    "        self.episodes_finished +=1\n",
    "        if (self.episodes_finished % num_agents) == 0:\n",
    "            self.num_episodes += 1\n",
    "            total_over_agents = 0\n",
    "            for i in range(num_agents):\n",
    "                total_over_agents += self.episode_scores[i][-1]\n",
    "            mean_score_over_agents = total_over_agents / num_agents    \n",
    "            self.last_100_scores.append(mean_score_over_agents)\n",
    "            self.mean_scores.append(mean_score_over_agents)\n",
    "            print(\"Finished %s episodes (%s cycles). mean_score_over_agents %s trailing %s\" % (self.num_episodes, (self.episodes_finished/num_agents), mean_score_over_agents, np.mean(self.last_100_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Main Program\n",
    "\n",
    "Run the cell below to get the average scores of 20 agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reach 100 episode trailing average of 30.00 in under 300 episodes.\n",
      "Rollout length: 500\n",
      "GRADIENT_CLIP 0.75\n",
      "PPO_RATIO_CLIP 0.1\n",
      "GAE_LAMBDA 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 episodes (1.0 cycles). mean_score_over_agents 0.0779999982566 trailing 0.0779999982566\n",
      "Finished 2 episodes (2.0 cycles). mean_score_over_agents 0.182499995921 trailing 0.130249997089\n",
      "Finished 3 episodes (3.0 cycles). mean_score_over_agents 0.353499992099 trailing 0.204666662092\n",
      "Finished 4 episodes (4.0 cycles). mean_score_over_agents 0.324999992736 trailing 0.234749994753\n",
      "Finished 5 episodes (5.0 cycles). mean_score_over_agents 0.529999988154 trailing 0.293799993433\n",
      "Finished 6 episodes (6.0 cycles). mean_score_over_agents 0.699999984354 trailing 0.36149999192\n",
      "Finished 7 episodes (7.0 cycles). mean_score_over_agents 0.637499985751 trailing 0.400928562467\n",
      "Finished 8 episodes (8.0 cycles). mean_score_over_agents 0.903999979794 trailing 0.463812489633\n",
      "Finished 9 episodes (9.0 cycles). mean_score_over_agents 0.795999982208 trailing 0.50072221103\n",
      "Finished 10 episodes (10.0 cycles). mean_score_over_agents 1.10299997535 trailing 0.560949987462\n",
      "Finished 11 episodes (11.0 cycles). mean_score_over_agents 0.766499982867 trailing 0.57963635068\n",
      "Finished 12 episodes (12.0 cycles). mean_score_over_agents 1.20049997317 trailing 0.631374985888\n",
      "Finished 13 episodes (13.0 cycles). mean_score_over_agents 1.33849997008 trailing 0.685769215441\n",
      "Finished 14 episodes (14.0 cycles). mean_score_over_agents 1.18199997358 trailing 0.721214269594\n",
      "Finished 15 episodes (15.0 cycles). mean_score_over_agents 1.59599996433 trailing 0.779533315909\n",
      "Finished 16 episodes (16.0 cycles). mean_score_over_agents 1.84599995874 trailing 0.846187481086\n",
      "Finished 17 episodes (17.0 cycles). mean_score_over_agents 1.77749996027 trailing 0.900970568097\n",
      "Finished 18 episodes (18.0 cycles). mean_score_over_agents 1.81099995952 trailing 0.951527756509\n",
      "Finished 19 episodes (19.0 cycles). mean_score_over_agents 1.92649995694 trailing 1.00284208285\n",
      "Finished 20 episodes (20.0 cycles). mean_score_over_agents 1.85149995862 trailing 1.04527497664\n",
      "Finished 21 episodes (21.0 cycles). mean_score_over_agents 1.91249995725 trailing 1.08657140428\n",
      "Finished 22 episodes (22.0 cycles). mean_score_over_agents 2.14699995201 trailing 1.13477270191\n",
      "Finished 23 episodes (23.0 cycles). mean_score_over_agents 2.01399995498 trailing 1.17299997378\n",
      "Finished 24 episodes (24.0 cycles). mean_score_over_agents 2.62949994123 trailing 1.23368747242\n",
      "Finished 25 episodes (25.0 cycles). mean_score_over_agents 2.18199995123 trailing 1.27161997158\n",
      "Finished 26 episodes (26.0 cycles). mean_score_over_agents 2.6529999407 trailing 1.32474997039\n",
      "Finished 27 episodes (27.0 cycles). mean_score_over_agents 2.47749994462 trailing 1.36744441388\n",
      "Finished 28 episodes (28.0 cycles). mean_score_over_agents 2.81299993712 trailing 1.41907139685\n",
      "Finished 29 episodes (29.0 cycles). mean_score_over_agents 3.48299992215 trailing 1.490241346\n",
      "Finished 30 episodes (30.0 cycles). mean_score_over_agents 2.67899994012 trailing 1.52986663247\n",
      "Finished 31 episodes (31.0 cycles). mean_score_over_agents 3.4539999228 trailing 1.59193544829\n",
      "Finished 32 episodes (32.0 cycles). mean_score_over_agents 3.39799992405 trailing 1.64837496316\n",
      "Finished 33 episodes (33.0 cycles). mean_score_over_agents 4.27099990454 trailing 1.72784844623\n",
      "Finished 34 episodes (34.0 cycles). mean_score_over_agents 3.43699992318 trailing 1.77811760731\n",
      "Finished 35 episodes (35.0 cycles). mean_score_over_agents 4.51199989915 trailing 1.85622852994\n",
      "Finished 36 episodes (36.0 cycles). mean_score_over_agents 4.68599989526 trailing 1.93483329009\n",
      "Finished 37 episodes (37.0 cycles). mean_score_over_agents 4.72699989434 trailing 2.01029725236\n",
      "Finished 38 episodes (38.0 cycles). mean_score_over_agents 5.2569998825 trailing 2.09573679526\n",
      "Finished 39 episodes (39.0 cycles). mean_score_over_agents 5.55249987589 trailing 2.18437174605\n",
      "Finished 40 episodes (40.0 cycles). mean_score_over_agents 6.00499986578 trailing 2.27988744904\n",
      "Finished 41 episodes (41.0 cycles). mean_score_over_agents 6.96649984429 trailing 2.39419506844\n",
      "Finished 42 episodes (42.0 cycles). mean_score_over_agents 6.67149985088 trailing 2.49603565849\n",
      "Finished 43 episodes (43.0 cycles). mean_score_over_agents 7.89349982357 trailing 2.62155808094\n",
      "Finished 44 episodes (44.0 cycles). mean_score_over_agents 8.12949981829 trailing 2.74673857497\n",
      "Finished 45 episodes (45.0 cycles). mean_score_over_agents 10.379499768 trailing 2.91635549037\n",
      "Finished 46 episodes (46.0 cycles). mean_score_over_agents 8.56999980845 trailing 3.03926080163\n",
      "Finished 47 episodes (47.0 cycles). mean_score_over_agents 10.4804997657 trailing 3.19758503491\n",
      "Finished 48 episodes (48.0 cycles). mean_score_over_agents 11.9614997326 trailing 3.38016659111\n",
      "Finished 49 episodes (49.0 cycles). mean_score_over_agents 14.5644996745 trailing 3.60841828669\n",
      "Finished 50 episodes (50.0 cycles). mean_score_over_agents 14.6919996716 trailing 3.83008991439\n",
      "Finished 51 episodes (51.0 cycles). mean_score_over_agents 16.0994996401 trailing 4.07066657568\n",
      "Finished 52 episodes (52.0 cycles). mean_score_over_agents 18.3829995891 trailing 4.34590374902\n"
     ]
    }
   ],
   "source": [
    "agent = MasterAgent(num_agents, state_size=state_size, action_size=action_size, seed=0, device = torch.device('cpu'))\n",
    "session = TrainingSession(agent, num_agents)\n",
    "scores = session.train_ppo(agent, 30.0)   # Do the training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Run the Policy\n",
    "The code below runs the policy that has previously been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run_ppo(agent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
